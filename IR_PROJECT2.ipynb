{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Untuk Project kali ini kami menggunakan beberapa Lbrary. Yang pertama CountVectorizer yang berguna untuk membentuk Dictionary dari term yang ada di dokumen. Lalu Numpy kami gunakan untuk membentuk vector untuk hasil perhitungan tf.idf. Lalu ada Math yang kami gunakan untuk proses perhitungan logaritma. Lalu Pandas untuk membentuk sebuah dataframe. Dan yang terakhir json yang kami gunakan untuk memanggil file dokumen dan relevant judgement kami yang berbentuk json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alien\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['articl', 'mon'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"ain\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"aren\", \"aren't\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\", \"couldn\", \"couldn't\", \"d\", \"did\", \"didn\", \"didn't\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"don't\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"hadn\", \"hadn't\", \"has\", \"hasn\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"i\", \"if\", \"in\", \"into\", \"is\", \"isn\", \"isn't\", \"it\", \"it's\", \"its\", \"itself\", \"just\", \"ll\", \"m\", \"ma\", \"me\", \"mightn\", \"mightn't\", \"more\", \"most\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"needn\", \"needn't\", \"no\", \"nor\", \"not\", \"now\", \"o\", \"of\", \"off\", \"on\", \"once\", \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"re\", \"s\", \"same\", \"shan\", \"shan't\", \"she\", \"she's\", \"should\", \"should've\", \"shouldn\", \"shouldn't\", \"so\", \"some\", \"such\", \"t\", \"than\", \"that\", \"that'll\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"ve\", \"very\", \"was\", \"wasn\", \"wasn't\", \"we\", \"were\", \"weren\", \"weren't\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\", \"whom\", \"why\", \"will\", \"with\", \"won\", \"won't\", \"wouldn\", \"wouldn't\", \"y\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"could\", \"he'd\", \"he'll\", \"he's\", \"here's\", \"how's\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"let's\", \"ought\", \"she'd\", \"she'll\", \"that's\", \"there's\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"what's\", \"when's\", \"where's\", \"who's\", \"why's\", \"would\", \"able\", \"abst\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"added\", \"adj\", \"affected\", \"affecting\", \"affects\", \"afterwards\", \"ah\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"among\", \"amongst\", \"announce\", \"another\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"apparently\", \"approximately\", \"arent\", \"arise\", \"around\", \"aside\", \"ask\", \"asking\", \"auth\", \"available\", \"away\", \"awfully\", \"b\", \"back\", \"became\", \"become\", \"becomes\", \"becoming\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"believe\", \"beside\", \"besides\", \"beyond\", \"biol\", \"brief\", \"briefly\", \"c\", \"ca\", \"came\", \"cannot\", \"can't\", \"cause\", \"causes\", \"certain\", \"certainly\", \"co\", \"com\", \"come\", \"comes\", \"contain\", \"containing\", \"contains\", \"couldnt\", \"date\", \"different\", \"done\", \"downwards\", \"due\", \"e\", \"ed\", \"edu\", \"effect\", \"eg\", \"eight\", \"eighty\", \"either\", \"else\", \"elsewhere\", \"end\", \"ending\", \"enough\", \"especially\", \"et\", \"etc\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"except\", \"f\", \"far\", \"ff\", \"fifth\", \"first\", \"five\", \"fix\", \"followed\", \"following\", \"follows\", \"former\", \"formerly\", \"forth\", \"found\", \"four\", \"furthermore\", \"g\", \"gave\", \"get\", \"gets\", \"getting\", \"give\", \"given\", \"gives\", \"giving\", \"go\", \"goes\", \"gone\", \"got\", \"gotten\", \"h\", \"happens\", \"hardly\", \"hed\", \"hence\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"hereupon\", \"hes\", \"hi\", \"hid\", \"hither\", \"home\", \"howbeit\", \"however\", \"hundred\", \"id\", \"ie\", \"im\", \"immediate\", \"immediately\", \"importance\", \"important\", \"inc\", \"indeed\", \"index\", \"information\", \"instead\", \"invention\", \"inward\", \"itd\", \"it'll\", \"j\", \"k\", \"keep\", \"keeps\", \"kept\", \"kg\", \"km\", \"know\", \"known\", \"knows\", \"l\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"least\", \"less\", \"lest\", \"let\", \"lets\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"'ll\", \"look\", \"looking\", \"looks\", \"ltd\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"million\", \"miss\", \"ml\", \"moreover\", \"mostly\", \"mr\", \"mrs\", \"much\", \"mug\", \"must\", \"n\", \"na\", \"name\", \"namely\", \"nay\", \"nd\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"nine\", \"ninety\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"normally\", \"nos\", \"noted\", \"nothing\", \"nowhere\", \"obtain\", \"obtained\", \"obviously\", \"often\", \"oh\", \"ok\", \"okay\", \"old\", \"omitted\", \"one\", \"ones\", \"onto\", \"ord\", \"others\", \"otherwise\", \"outside\", \"overall\", \"owing\", \"p\", \"page\", \"pages\", \"part\", \"particular\", \"particularly\", \"past\", \"per\", \"perhaps\", \"placed\", \"please\", \"plus\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"predominantly\", \"present\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"put\", \"q\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"ran\", \"rather\", \"rd\", \"readily\", \"really\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"right\", \"run\", \"said\", \"saw\", \"say\", \"saying\", \"says\", \"sec\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sent\", \"seven\", \"several\", \"shall\", \"shed\", \"shes\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"six\", \"slightly\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"take\", \"taken\", \"taking\", \"tell\", \"tends\", \"th\", \"thank\", \"thanks\", \"thanx\", \"thats\", \"that've\", \"thence\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"thereto\", \"thereupon\", \"there've\", \"theyd\", \"theyre\", \"think\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"throug\", \"throughout\", \"thru\", \"thus\", \"til\", \"tip\", \"together\", \"took\", \"toward\", \"towards\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"twice\", \"two\", \"u\", \"un\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"unto\", \"upon\", \"ups\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"v\", \"value\", \"various\", \"'ve\", \"via\", \"viz\", \"vol\", \"vols\", \"vs\", \"w\", \"want\", \"wants\", \"wasnt\", \"way\", \"wed\", \"welcome\", \"went\", \"werent\", \"whatever\", \"what'll\", \"whats\", \"whence\", \"whenever\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"whereupon\", \"wherever\", \"whether\", \"whim\", \"whither\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whomever\", \"whos\", \"whose\", \"widely\", \"willing\", \"wish\", \"within\", \"without\", \"wont\", \"words\", \"world\", \"wouldnt\", \"www\", \"x\", \"yes\", \"yet\", \"youd\", \"youre\", \"z\", \"zero\", \"a's\", \"ain't\", \"allow\", \"allows\", \"apart\", \"appear\", \"appreciate\", \"appropriate\", \"associated\", \"best\", \"better\", \"c'mon\", \"c's\", \"cant\", \"changes\", \"clearly\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"corresponding\", \"course\", \"currently\", \"definitely\", \"described\", \"despite\", \"entirely\", \"exactly\", \"example\", \"going\", \"greetings\", \"hello\", \"help\", \"hopefully\", \"ignored\", \"inasmuch\", \"indicate\", \"indicated\", \"indicates\", \"inner\", \"insofar\", \"it'd\", \"keep\", \"keeps\", \"novel\", \"presumably\", \"reasonably\", \"second\", \"secondly\", \"sensible\", \"serious\", \"seriously\", \"sure\", \"t's\", \"third\", \"thorough\", \"thoroughly\", \"three\", \"well\", \"wonder\", \"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\", \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\", \"co\", \"op\", \"research-articl\", \"pagecount\", \"cit\", \"ibid\", \"les\", \"le\", \"au\", \"que\", \"est\", \"pas\", \"vol\", \"el\", \"los\", \"pp\", \"u201d\", \"well-b\", \"http\", \"volumtype\", \"par\", \"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"ac\", \"ad\", \"ae\", \"af\", \"ag\", \"aj\", \"al\", \"an\", \"ao\", \"ap\", \"ar\", \"av\", \"aw\", \"ax\", \"ay\", \"az\", \"b1\", \"b2\", \"b3\", \"ba\", \"bc\", \"bd\", \"be\", \"bi\", \"bj\", \"bk\", \"bl\", \"bn\", \"bp\", \"br\", \"bs\", \"bt\", \"bu\", \"bx\", \"c1\", \"c2\", \"c3\", \"cc\", \"cd\", \"ce\", \"cf\", \"cg\", \"ch\", \"ci\", \"cj\", \"cl\", \"cm\", \"cn\", \"cp\", \"cq\", \"cr\", \"cs\", \"ct\", \"cu\", \"cv\", \"cx\", \"cy\", \"cz\", \"d2\", \"da\", \"dc\", \"dd\", \"de\", \"df\", \"di\", \"dj\", \"dk\", \"dl\", \"do\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"dx\", \"dy\", \"e2\", \"e3\", \"ea\", \"ec\", \"ed\", \"ee\", \"ef\", \"ei\", \"ej\", \"el\", \"em\", \"en\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"et\", \"eu\", \"ev\", \"ex\", \"ey\", \"f2\", \"fa\", \"fc\", \"ff\", \"fi\", \"fj\", \"fl\", \"fn\", \"fo\", \"fr\", \"fs\", \"ft\", \"fu\", \"fy\", \"ga\", \"ge\", \"gi\", \"gj\", \"gl\", \"go\", \"gr\", \"gs\", \"gy\", \"h2\", \"h3\", \"hh\", \"hi\", \"hj\", \"ho\", \"hr\", \"hs\", \"hu\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ic\", \"ie\", \"ig\", \"ih\", \"ii\", \"ij\", \"il\", \"in\", \"io\", \"ip\", \"iq\", \"ir\", \"iv\", \"ix\", \"iy\", \"iz\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"ke\", \"kg\", \"kj\", \"km\", \"ko\", \"l2\", \"la\", \"lb\", \"lc\", \"lf\", \"lj\", \"ln\", \"lo\", \"lr\", \"ls\", \"lt\", \"m2\", \"ml\", \"mn\", \"mo\", \"ms\", \"mt\", \"mu\", \"n2\", \"nc\", \"nd\", \"ne\", \"ng\", \"ni\", \"nj\", \"nl\", \"nn\", \"nr\", \"ns\", \"nt\", \"ny\", \"oa\", \"ob\", \"oc\", \"od\", \"of\", \"og\", \"oi\", \"oj\", \"ol\", \"om\", \"on\", \"oo\", \"oq\", \"or\", \"os\", \"ot\", \"ou\", \"ow\", \"ox\", \"oz\", \"p1\", \"p2\", \"p3\", \"pc\", \"pd\", \"pe\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"pm\", \"pn\", \"po\", \"pq\", \"pr\", \"ps\", \"pt\", \"pu\", \"py\", \"qj\", \"qu\", \"r2\", \"ra\", \"rc\", \"rd\", \"rf\", \"rh\", \"ri\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"rv\", \"ry\", \"s2\", \"sa\", \"sc\", \"sd\", \"se\", \"sf\", \"si\", \"sj\", \"sl\", \"sm\", \"sn\", \"sp\", \"sq\", \"sr\", \"ss\", \"st\", \"sy\", \"sz\", \"t1\", \"t2\", \"t3\", \"tb\", \"tc\", \"td\", \"te\", \"tf\", \"th\", \"ti\", \"tj\", \"tl\", \"tm\", \"tn\", \"tp\", \"tq\", \"tr\", \"ts\", \"tt\", \"tv\", \"tx\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"uo\", \"ur\", \"ut\", \"va\", \"wa\", \"vd\", \"wi\", \"vj\", \"vo\", \"wo\", \"vq\", \"vt\", \"vu\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y2\", \"yj\", \"yl\", \"yr\", \"ys\", \"yt\", \"zi\", \"zz\"]\n",
    "documents = pd.read_json(\"cranfield.json\")\n",
    "import re\n",
    "\n",
    "documents['text'] = documents['title'] + documents['body']\n",
    "doc = documents['text']\n",
    "\n",
    "cv=CountVectorizer(stop_words=stopwords)\n",
    "word_count_vector=cv.fit_transform(doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kami juga mempunyai sebuah array yang menyimpan kata-kata stopwords atau kata-kata yang tidak berpengaruh banyak dalam dokument. Lalu kami memanggil dataset kami yang bernama \"cranfield.json\" lalu menyimpannya pada sebuah dataframe. Lalu setelah itu kami membuat sebuah key baru yang bernama \"text\". Dimana value dari key ini merupakan penggabungan dari value key \"title\" dan value key \"body\". Lalu kami menyimpan value dari key \"text\" ke sebuah list baru yang bernama \"doc\". Dimana doc ini nanti akan digunakan untuk proses perhitungan tf.idf.\n",
    "\n",
    "Lalu menggunakan CountVectorizer kami membuat kamus term yang ada pada dataset dengan menghiraukan kata-kata yang termasuk stopword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# megnhitung jumlah term di setiap dokumen\n",
    "def termFrequency(term, document):\n",
    "    normalizeDocument = document.lower().split()\n",
    "    return normalizeDocument.count(term.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk menghitung jumlah setiap term pada dokumen kami membuat sebuah fungsi yang bernama \"termFrequency\". Pada fungsi ini kami menghitung jumlah term pada dokumen dengan menggunakan fungsi count()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#megnhitung idf tiap document serta mengecek apakah suatu term merupakan stopword\n",
    "def is_stop_word(term):\n",
    "    return (term in stopwords)\n",
    "\n",
    "\n",
    "def inverseDocumentFrequency(term, documents):\n",
    "    count = 0\n",
    "    for doc in documents:\n",
    "        if term.lower() in doc.lower().split():\n",
    "             if(is_stop_word(term)):\n",
    "                break\n",
    "             else:\n",
    "                count += 1\n",
    "    if count > 0:\n",
    "        return math.log10(float(len(documents))/count)\n",
    "    else:\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Untuk menghitung IDF tiap - tiap dokumen terhadapat term yang ada pada Query. Kami membuat fungsi yang kami namankan \"inverseDocumentFrequency\". Fungsi ini mempunyai parameter term dan documents. Dimana term ini adalah term yang akan dihitung nilai idfnya dan documents adalah koleksi dokumen atau dataset yang kami pakai dalam project kali ini.\n",
    "\n",
    "Lalu, untuk metode yang kami gunakan untuk menghitung IDF ini. Pertama kami mempunyai sebuah variable yang bernama \"count\", variable ini berguna untuk menghitung jumlah dokumen yang mengandung term. Lalu kami mengecek apakah term tersebut ada dalam dokumen atau tidak, jika ada kami mengecek apakah term tersebut termasuk dalam stopwords atau tidak. Jika termasuk maka kami akan memanggil fungsi break dan menghentikan proses pengecekan. Namun jika term tersebut bukan termasuk stopwords maka variable \"count\" akan bertambah satu. Lalu setelah proses pengecekan term di dokumen selesai. Kami akan mengecek apakah \"count\" bernilai lebih dari 1. Jika ya maka fungsi akan mengembalikan nilai dengan perhitungan log(d/count) dimana d adalah jumlah dokumen pada dataset dan count adalah jumlah dokumen yang mengandung term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tf-idf of a term in a document\n",
    "def tf_idf(term, document, documents):\n",
    "    tf = termFrequency(term, document)\n",
    "    idf = inverseDocumentFrequency(term, documents)\n",
    "    return tf*idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk menghitung TD.IDF tiap dokumen terhadap term yang berada dalam query kami membuat sebuah fungsi yang bernama \"tf_idf\". Pada fungsi ini kami akan memanggil fungsi termFrequency untuk menghitung tf dan fungsi inverseDocumentFrequency untung menghitung idfnya. Lalu fungsi ini akan mengembalikan nilai tf*idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cari_query(n):\n",
    "    print(\"\\n\")\n",
    "    query=str(input('Masukkan Query: '))\n",
    "    query_len = len(query.split())\n",
    "    print(query_len)\n",
    "    # \"what are the structural and aeroelastic problems associated with flight of high speed aircraft\"\n",
    "    print(\"\\n\")\n",
    "    tf_idf_query=np.zeros((len(query.split()), 1))\n",
    "    i=0;\n",
    "    for term in query.lower().split():                    \n",
    "            tf_idf_query[i] = tf_idf(term, query, doc)    #Menghitung tf_idf untuk setiap term pada query lalu measukkannya kedalam vektor tf_idf_query\n",
    "            i +=1\n",
    "\n",
    "    tf_idf_matrix = generateVectors(query, doc)  #Menggenerate tf_idf untuk setiap term pada query untuk seluruh dokumen lalu measukkannya kedalam vektor tf_idf_matrix\n",
    "   \n",
    "    import operator\n",
    "\n",
    "    sc = {};\n",
    "    scs=0;\n",
    "    i=0;\n",
    "    j=0;\n",
    "    \n",
    "    for s in range(len(tf_idf_matrix[0])):\n",
    "        for i in range(len(tf_idf_matrix)):\n",
    "            scs += (tf_idf_query[i] * tf_idf_matrix[i][j])\n",
    "        sc[s+1] = scs\n",
    "        scs=0\n",
    "        j += 1\n",
    "    dd= sorted(sc.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    print(\"Hasil Dokumen Pencarian (ID, Similarity) Sebelum Rocchio\\n\")\n",
    "    for i in range (n):\n",
    "        print(i+1, dd[i])\n",
    "    \n",
    "    # IMPLEMENTASI ROCCHIO\n",
    "    indexs=[]\n",
    "    w, h = 10, query_len;\n",
    "    vec_rev = 0; ##Variabel Untuk Menyimpan penjumlahan tiap dimensi dari dokumen relevan\n",
    "    vec_unrev = 0; ##Variabel Untuk Menyimpan penjumlahan tiap dimensi dari dokumen unrelevan\n",
    "    vector_relevant = [] ##Vector dokumen relevan yang sudah dibagi 10\n",
    "    vector_unrelevant= [] ##Vector dokumen unrelevan yang sudah dibagi 10\n",
    "    ##Proses Mengisi Vektor Dokumen relevan dan Tidak Relevan\n",
    "    for j in range(query_len): ##Membuat for sebanyak kata dalam query (Dimensi dari Vektor)\n",
    "        for i in range(20):##Membuat for sebanyak 20 dokumen (Ranking 1-20) ranking 1-10 dianggap relevan, rangking 11-20 dianggap tidak relevan\n",
    "            indexs.append(dd[i][0]) ##Mengambil ID Dokumen Rangking 1-20 lalu menyimpannya di dalam variabel indexs\n",
    "            if(i<10):\n",
    "                vec_rev = vec_rev+tf_idf_matrix[j][indexs[i]-1];\n",
    "                if(i==9):\n",
    "                    vector_relevant.insert(j, vec_rev/10)\n",
    "            if(i>9):\n",
    "                vec_unrev = vec_unrev+tf_idf_matrix[j][indexs[i]-1];\n",
    "                if(i==19):\n",
    "                    vector_unrelevant.insert(j, vec_unrev/10)\n",
    "        vec_rev=0\n",
    "        vec_unrev = 0;\n",
    "\n",
    "    tf_idf_query_new =np.zeros((len(query.split()), 1))\n",
    "    for j in range(query_len):\n",
    "        s = tf_idf_query[j]+vector_relevant[j]-vector_unrelevant[j]\n",
    "        tf_idf_query_new[j] = s\n",
    "\n",
    "    k=0;\n",
    "    for s in range(len(tf_idf_matrix[0])):\n",
    "        for i in range(len(tf_idf_matrix)):\n",
    "            scs += (tf_idf_query_new[i] * tf_idf_matrix[i][k])\n",
    "        sc[s+1] = scs\n",
    "        scs=0\n",
    "        k += 1\n",
    "    ds= sorted(sc.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    print(\"\\n\\n\\nHasil Dokumen Pencarian (ID, Similarity) Setelah Rocchio\\n\")\n",
    "    for i in range (n):\n",
    "        print(i+1, ds[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Untuk fitur pencarian dengan query yang sesuai masukan user, kami membuat sebuh fungsi yang bernama cari_query. Dimana fungsi ini mempunyai paramter n, dimana n ini adalah jumlah dokumen yang akan ditampilkan pada hasil pencarian.\n",
    "\n",
    "Pertama-tama kami akan meminta user untuk menginput query. Lalu setelah itu, dengan menggunakan fungsi np.zeros kami membuat sebuah vektor dengan dimensi sesuai dengan banyak kata dalam query. Lalu setelah itu kami akan menghitung tf_idf dari setiap term yang ada pada query lalu menyimpannya pada vektor tf_idf_query.\n",
    "\n",
    "Lalu setelah itu kami membentuk sebuah variable baru yang bernama tf_idf_matrix. tf_idf_matrix ini berisi vektor dari tf_idf setiap dokumen terhadap term yang ada pada query dengan memanggil fungsi generateVectors().\n",
    "\n",
    "Lalu setelah itu kami menghitung sc dari query terhadap tiap-tiap dokumen lalu menyimpan nilai sc tersebut kedalam sebuah list yang bernama sc.\n",
    "\n",
    "Lalu yang kami menampilkan ranking dokumen dari hasil query tersebut sebelum query diperbaharui.\n",
    "\n",
    "Setelah itu kami akan mengambil id Dokumen dengan peringkat 20 Teratas. Lalu kami menyimpan vector dari 20 dokumen tersebut Kedalam dua buah variabel vector. Dokumen ranking 1-10 kami simpan ke dalam vector dokumen relevan, lalu ranking 11-20 kami simpan ke vector dokumen tidak relevant. Setelah itu kami melakukan perhitungan Rocchio. Lalu setelah itu kami melakukan perhitungan SC lagi dengan vector query yang baru. Lalu kami menampilkan rangking dokumen dengan query setelah algoritma rocchio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevant_judgement():\n",
    "    rel_judge = pd.read_json(\"cranqrel.json\")\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    print(\"1. Lihat Semua Query Relevant Judgement\")\n",
    "    print(\"2. Lihat Semua Hasil Relevant Judgment\")\n",
    "    print(\"3. Lihat Hasil Query Relevant Judgement Dengan ID Spesifik\")\n",
    "    choice=str(input(\"Pilih Menu : \"))\n",
    "    if(choice=='1'):\n",
    "        f = open('cran.qry', 'r')\n",
    "        for line in f:\n",
    "            string_list = line.split()\n",
    "            if(string_list [0] == '.W'):\n",
    "                continue\n",
    "            print(line)\n",
    "        f.close()\n",
    "    \n",
    "    elif(choice=='2'):\n",
    "        f = open('cranqrel.json', 'r')\n",
    "        for line in f:\n",
    "            print(line)\n",
    "        f.close()\n",
    "        \n",
    "    elif(choice=='3'):\n",
    "        n=int(input(\"Masukkan id Query : \"))\n",
    "        df=pd.read_json(\"cranqrel.json\")\n",
    "        print(df.loc[df[\"query_num\"]  == n])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Lalu kami juga mempunyai fungsi relevant_judgement(). Dimana fungsi ini berfungsi untuk menampilkan List Query yang ada pada Relevant Judgement, Menampilkan Semua Hasil Query dari Relevant Judgement serta menampilkan Hasil Query dari Relevant Judgement dengan ID Query tertentu.\n",
    "\n",
    "Untuk menampilkan List Query yang ada pada Relevant Judgement kami memanggil sebuah file yang bernama cran.qry. File ini berisi kumpulan query relevant judgement.Lalu kami mengecek apakah kata awal dari tiap line adalah \".W\" atau bukan, Jika bukan makan akan ditampilkan.\n",
    "\n",
    "Untuk Menampilkan Semua Hasil Query dari Relevant Judgement kami memanggil sebuah file yang bernama cranqrel.json lalu menyimpannya pada variable f. File ini berisi hasil ranking dari query yang ada pada relevant judgement. Lalu kami menampilkannya.\n",
    "\n",
    "Untuk menampilkan Hasil Query dari Relevant Judgement dengan ID Query tertentu, pertama kami meminta user untuk menginputkan id query. Lalu setelah itu kami membuat dataframe yang berisikan file cranqrel.json. Lalu kami mencetak dataframe tersebut dengan ketentuan \"query_num\" sama dengan n. Dimana n adalah ID yang diinputkan oleh user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateVectors(query, documents):\n",
    "    tf_idf_matrix = np.zeros((len(query.split()), len(documents)))\n",
    "    for i, s in enumerate(query.lower().split()):\n",
    "        idf = inverseDocumentFrequency(s, documents)\n",
    "        for j,doc in enumerate(documents):\n",
    "            tf_idf_matrix[i][j] = idf * termFrequency(s, doc)\n",
    "    return tf_idf_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Untuk mengenerate vektor tf_idf_matrix yang mana berisi tf_idf dari tiap dokumen terhadap term yang ada di query kami membuat fungsi generateVectors(). tf_idf_matrix akan mempunyai dimensi panjang query x total dokumen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " \n",
      "\n",
      "1. Masukkan Query\n",
      "2. Atur Jumlah Hasil Query\n",
      "3. Lihat Relevant Judgement\n",
      "4. Keluar\n",
      "Pilihan : 1\n",
      "\n",
      "\n",
      "Masukkan Query: what are the structural and aeroelastic problems associated with flight of high speed aircraft\n",
      "14\n",
      "\n",
      "\n",
      "Hasil Dokumen Pencarian (ID, Similarity) Sebelum Rocchio\n",
      "\n",
      "1 (12, array([31.1771161]))\n",
      "2 (51, array([29.40019238]))\n",
      "3 (792, array([22.72424065]))\n",
      "4 (14, array([18.49925505]))\n",
      "5 (746, array([18.29559904]))\n",
      "6 (884, array([15.39331958]))\n",
      "7 (1169, array([14.46867849]))\n",
      "8 (781, array([14.20973092]))\n",
      "9 (1147, array([13.79330453]))\n",
      "10 (883, array([12.82672337]))\n",
      "\n",
      "\n",
      "\n",
      "Hasil Dokumen Pencarian (ID, Similarity) Setelah Rocchio\n",
      "\n",
      "1 (12, array([53.3129653]))\n",
      "2 (51, array([42.27242409]))\n",
      "3 (746, array([32.91665042]))\n",
      "4 (14, array([32.77763311]))\n",
      "5 (792, array([32.22011409]))\n",
      "6 (781, array([25.57751566]))\n",
      "7 (884, array([24.11912095]))\n",
      "8 (1147, array([23.81196344]))\n",
      "9 (606, array([20.22312601]))\n",
      "10 (1169, array([20.06549682]))\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "1. Masukkan Query\n",
      "2. Atur Jumlah Hasil Query\n",
      "3. Lihat Relevant Judgement\n",
      "4. Keluar\n",
      "Pilihan : 4\n",
      "Dadahhh\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n=10;\n",
    "pilihan='0'\n",
    "\n",
    "while(not pilihan == '4'):\n",
    "    print(\"\\n \\n \\n\")\n",
    "    print(\"1. Masukkan Query\")\n",
    "    print(\"2. Atur Jumlah Hasil Query\")\n",
    "    print(\"3. Lihat Relevant Judgement\")\n",
    "    print(\"4. Keluar\")\n",
    "    \n",
    "    pilihan=str(input(\"Pilihan : \"))\n",
    "\n",
    "    if(pilihan == '1'):\n",
    "        cari_query(n)\n",
    "\n",
    "    elif(pilihan == '2'):\n",
    "        n=int(input('Masukkan Jumlah Hasil Query Yang Akan Ditampilkan: '))\n",
    "        print(\"Berhasil Mengubah Hasil Jumlah query menjadi \" + str(n))\n",
    "        \n",
    "    elif(pilihan == '3'):\n",
    "        relevant_judgement()\n",
    "        \n",
    "    elif(pilihan == '4'):\n",
    "        print(\"Dadahhh\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lalu ini adalah menu main dari aplikasi kami. Akan ada 4 menu. DImana menu pertama adalah Mencari dengan query custom, Menu kedua adalah user mengatur jumlah dokumen yang akan ditampilkan, dan menu ketiga adalah menu Relevant Judgement.\n",
    "\n",
    "Pada awal program kami mempunya variable n yang bernilai 10. Dimana n ini adalah jumlah dokumen yang akan ditampilkan untuk hasil query. Dan variabel pilihan yang bernilai '0'. Dimana pilihan ini adalah variabel yang digunakan unser untuk memilih menu.\n",
    "\n",
    "Jika user memilih menu pertama, maka program akan memanggil fungsi cari_query dengan membawa n untuk parameter.\n",
    "\n",
    "Jika user memilih menu kedua, maka program akan meminta user untuk memasukkan jumlah dokumen yang akan ditampilkan lalu menyimpannya pada n.\n",
    "\n",
    "Jika user memilih menu ketiga, maka program akan memanggil fungsi relevant_judgement.\n",
    "\n",
    "Jika user memilih menu keempat maka user akan menampilkan \"Dadahh\" dan program akan selesai."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
